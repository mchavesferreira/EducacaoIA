# Transformers



---

### üß† O que √© *Transformer*?

**Transformer** √© uma **arquitetura de rede neural** criada pelo time do Google em 2017. Ela revolucionou a √°rea de **intelig√™ncia artificial** e **processamento de linguagem natural (NLP)**. √â a base de modelos como **GPT**, **BERT**, e outros modelos modernos de linguagem.

---

### üß© Como funciona, de forma simples?

Antes do Transformer, os modelos usavam sequ√™ncias (um por um, como uma fila). O Transformer, por outro lado, olha **todas as palavras ao mesmo tempo** e decide **quais palavras s√£o mais importantes** em cada contexto.

Imagine que voc√™ est√° lendo a frase:

> ‚ÄúO cachorro mordeu o carteiro porque ele estava assustado.‚Äù

Quem estava assustado? O cachorro ou o carteiro? ü§î
O Transformer tenta entender isso olhando o **contexto completo** da frase, e n√£o s√≥ palavra por palavra.

---

### üõ†Ô∏è Pe√ßas principais do Transformer

1. **Attention (aten√ß√£o)**:
   Esta √© a m√°gica! O mecanismo de *attention* faz com que o modelo ‚Äúpreste aten√ß√£o‚Äù mais nas palavras que realmente importam.
   Ex: Se a frase fala sobre um "gato", o modelo vai focar mais em palavras relacionadas a ele.

2. **Encoder e Decoder**:

   * O **Encoder** l√™ e entende a entrada (por exemplo, uma frase).
   * O **Decoder** usa essa informa√ß√£o para gerar uma resposta (por exemplo, traduzir ou responder).

   No caso do **GPT**, s√≥ se usa o **Decoder**, porque ele √© um modelo de gera√ß√£o de texto.

---

### Visualiza√ß√£o gr√°fica sobre transformes

*  https://bbycroft.net/llm

*  https://poloclub.github.io/transformer-explainer/


### üéØ Por que o Transformer √© t√£o poderoso?

* Ele entende o **contexto** de forma mais profunda.
* Ele √© mais r√°pido para treinar (usa paralelismo).
* Ele funciona muito bem com **textos longos**.

---

Se quiser, posso mostrar um exemplo visual de como o "attention" funciona ou comparar Transformer com modelos antigos. Quer ver isso?

O modelo GPT (Generative Pre-trained Transformer) √© baseado na arquitetura Transformer, que foi originalmente proposta em um artigo chamado *Attention is All You Need* (Vaswani et al., 2017).

Os Transformers, como o usado no GPT, utilizam um mecanismo de aten√ß√£o que permite ao modelo focar em diferentes partes de uma sequ√™ncia de entrada ao fazer previs√µes, em vez de processar os dados sequencialmente, como fazem redes neurais recorrentes (RNNs). Isso torna o Transformer muito eficiente para lidar com grandes volumes de dados, como textos.

A arquitetura **Transformer** √© composta por duas partes principais: o **Encoder** e o **Decoder**. No caso de modelos como o **GPT**, apenas a parte do **Decoder** √© utilizada. Vamos passar por cada uma dessas etapas e detalhar os componentes principais de um Transformer.

### 1. **Estrutura B√°sica de um Transformer**

O Transformer, conforme proposto por Vaswani et al. (2017), √© composto por **camadas empilhadas de Encoder** e **camadas empilhadas de Decoder**. A arquitetura b√°sica √©:

* **Encoder** (apenas utilizado em modelos como BERT, T5, etc.)

  * Recebe a entrada (texto) e gera representa√ß√µes internas.
* **Decoder** (utilizado no GPT, entre outros)

  * Gera a sa√≠da (como texto gerado), utilizando as representa√ß√µes do Encoder (no caso do GPT, n√£o h√° um Encoder externo, mas a arquitetura de Decoder √© usada para prever a sequ√™ncia de sa√≠da).

Para modelos como o **GPT**, utilizamos apenas o **Decoder**, que √© empilhado v√°rias vezes. A sequ√™ncia de opera√ß√µes dentro de cada bloco de Decoder √©:

### 2. **Componentes principais do Transformer**

#### 2.1 **Embedding**

* **Token Embedding**: Cada palavra (ou subpalavra) da sequ√™ncia de entrada √© mapeada para um vetor de n√∫meros. Esses vetores s√£o chamados de embeddings.
* **Positional Encoding**: Como o Transformer n√£o tem estrutura sequencial como as RNNs, precisamos de uma maneira de adicionar informa√ß√µes sobre a posi√ß√£o das palavras na sequ√™ncia. O Positional Encoding fornece isso, com um vetor para cada posi√ß√£o na sequ√™ncia, que √© somado aos embeddings dos tokens.

#### 2.2 **Camada de Aten√ß√£o (Self-Attention)**

A **Aten√ß√£o** √© o n√∫cleo da arquitetura Transformer. A cada camada, o modelo calcula a rela√ß√£o de cada palavra em rela√ß√£o a todas as outras na sequ√™ncia. Para isso, usamos tr√™s componentes:

* **Queries (Q)**, **Keys (K)** e **Values (V)**: Para cada token de entrada, geramos tr√™s vetores diferentes (Q, K e V). Esses vetores s√£o obtidos atrav√©s de multiplica√ß√£o da matriz de entrada pelos pesos aprendidos durante o treinamento.

* **Aten√ß√£o Escalonada (Scaled Dot-Product Attention)**: A aten√ß√£o calcula uma pontua√ß√£o entre todas as palavras de entrada, usando os vetores **Query** e **Key**. O resultado √© uma pondera√ß√£o que indica a import√¢ncia de cada palavra para a palavra de interesse.

  A f√≥rmula da aten√ß√£o √© dada por:
  [
  \text{Aten√ß√£o}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
  ]
  Onde:

  * ( Q ) √© o vetor de consulta (Query).
  * ( K ) √© o vetor de chave (Key).
  * ( V ) √© o vetor de valor (Value).
  * ( d_k ) √© a dimensionalidade do vetor de chave.

A aten√ß√£o permite ao modelo capturar depend√™ncias de longo alcance entre as palavras, sem precisar de uma ordem sequencial expl√≠cita.

#### 2.3 **Aten√ß√£o Multi-Cabe√ßa (Multi-Head Attention)**

Em vez de usar apenas uma √∫nica aten√ß√£o, o Transformer utiliza m√∫ltiplas cabe√ßas de aten√ß√£o em paralelo. Isso permite que o modelo capture diferentes aspectos da rela√ß√£o entre palavras de diferentes "perspectivas". As m√∫ltiplas cabe√ßas de aten√ß√£o geram diferentes representa√ß√µes, que depois s√£o concatenadas e transformadas.

#### 2.4 **Camada Feed-Forward**

Ap√≥s a camada de aten√ß√£o, cada token passa por uma camada feed-forward totalmente conectada. Esse componente √© composto por duas camadas lineares (densas) com uma fun√ß√£o de ativa√ß√£o no meio (tipicamente ReLU). A camada feed-forward √© aplicada de forma independente a cada token.

A f√≥rmula da camada feed-forward √©:
[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
]
Onde:

* ( x ) √© a entrada da camada.
* ( W_1, W_2 ) s√£o as matrizes de pesos.
* ( b_1, b_2 ) s√£o os vieses.

#### 2.5 **Normaliza√ß√£o e Res√≠duos**

* **Layer Normalization**: Ap√≥s as opera√ß√µes de aten√ß√£o e feed-forward, as sa√≠das passam por uma normaliza√ß√£o de camada para estabilizar o treinamento.
* **Residual Connection**: Para garantir que o modelo n√£o perca informa√ß√µes importantes durante o processamento, √© adicionada uma conex√£o residual entre a entrada e a sa√≠da de cada subcamada (aten√ß√£o ou feed-forward). Ou seja, a entrada original de cada subcamada √© somada √† sua sa√≠da antes de ser passada para a pr√≥xima subcamada.

#### 2.6 **Camada de Sa√≠da**

No caso do GPT (que √© um modelo de gera√ß√£o de texto), a camada final do modelo √© uma **camada de proje√ß√£o linear** que mapeia as representa√ß√µes internas para uma distribui√ß√£o de probabilidade sobre o vocabul√°rio. Essa camada √© usada para prever o pr√≥ximo token na sequ√™ncia.

### 3. **Fluxo de Dados no Transformer**

1. **Entrada**: O modelo recebe uma sequ√™ncia de palavras (tokens).
2. **Embeddings + Positional Encoding**: Cada palavra √© transformada em um vetor denso atrav√©s do embedding, e a posi√ß√£o √© codificada atrav√©s do positional encoding.
3. **Camadas de Decoder** (empilhadas):

   * **Aten√ß√£o Multi-Cabe√ßa**: Cada token da sequ√™ncia √© processado em rela√ß√£o aos outros tokens.
   * **Camada Feed-Forward**: Ap√≥s a aten√ß√£o, cada token passa por uma rede neural feed-forward.
   * **Normaliza√ß√£o e Residual**: A entrada da camada √© somada √† sua sa√≠da e normalizada.
4. **Sa√≠da**: O modelo gera uma distribui√ß√£o de probabilidade para o pr√≥ximo token, que √© usado para fazer previs√µes ou gerar texto.

### 4. **Resumo do Fluxo em um Transformer**

* A entrada (sequ√™ncia de tokens) √© convertida em embeddings.
* A cada camada de Decoder, o modelo realiza:

  * **Aten√ß√£o** (multi-cabe√ßa) para calcular as depend√™ncias entre os tokens.
  * **Feed-forward** para processar as representa√ß√µes dos tokens.
  * **Normaliza√ß√£o e residuals** para estabilidade no treinamento.
* O modelo gera as sa√≠das (como probabilidades para o pr√≥ximo token, no caso de gera√ß√£o de texto).

### Resumo para o GPT

O **GPT** (Generative Pretrained Transformer) utiliza apenas o **Decoder** do Transformer. Durante o pr√©-treinamento, ele aprende a prever o pr√≥ximo token em uma sequ√™ncia, com base no contexto anterior. A estrutura de cada bloco do Decoder √© composta por:

* **Aten√ß√£o Multi-Cabe√ßa**,
* **Camada Feed-Forward**,
* **Normaliza√ß√£o e Conex√µes Residuais**.

Esse fluxo √© repetido por v√°rias camadas de Decoder para gerar representa√ß√µes cada vez mais complexas e, finalmente, prever a sequ√™ncia de sa√≠da.

## prompt

√ìtimas perguntas! Vamos esclarecer esses pontos em detalhes.

### 1. **Uso do Encoder e Decoder no GPT-3.5**

Quando as not√≠cias falam que o **ChatGPT 3.5** ou outro modelo GPT "consultou bilh√µes de informa√ß√µes da internet" para seu treinamento, isso se refere ao processo de **pr√©-treinamento** do modelo, onde ele foi alimentado com uma grande quantidade de dados textuais extra√≠dos da web, livros, artigos, entre outros. Durante esse treinamento, o GPT **n√£o usou o Encoder**, mas sim o **Decoder** da arquitetura Transformer.

Aqui est√° o processo de treinamento de forma simplificada:

* **Pr√©-Treinamento**: Durante o treinamento do GPT-3.5, o modelo foi alimentado com enormes volumes de dados textuais. O objetivo era fazer com que ele aprendesse a prever o pr√≥ximo token (palavra, por exemplo) em uma sequ√™ncia de texto. Esse √© um processo **autoregressivo** (onde o modelo usa os tokens anteriores para prever o pr√≥ximo). Para isso, ele usa **somente o Decoder** da arquitetura Transformer.

  * **Decoder**: Durante o treinamento, o modelo GPT-3.5 √© alimentado com um texto e, com base nesse texto, ele tenta prever o pr√≥ximo token. O modelo √© treinado para minimizar a diferen√ßa entre a previs√£o que ele fez e o token real da sequ√™ncia.
* **Pesos e Par√¢metros**: Durante esse treinamento, o modelo aprende **pesos** para as opera√ß√µes dentro do Decoder (como as camadas de aten√ß√£o e feed-forward), que s√£o ajustados para capturar padr√µes de linguagem e associa√ß√µes entre palavras. Esses pesos s√£o fundamentais para o modelo e s√£o o que ele utiliza para gerar respostas durante a infer√™ncia.

  Ou seja, **o GPT-3.5 n√£o usou o Encoder** para esse treinamento. Ele usou **apenas o Decoder** para aprender a prever sequ√™ncias de texto com base em um contexto dado.

### 2. **O que representa o "prompt" em um modelo LLM (Large Language Model)?**

No contexto de modelos de linguagem grandes como o **GPT-3.5**, o **prompt** √© o texto de entrada que voc√™ fornece ao modelo para gerar uma resposta ou realizar uma tarefa. Ele pode ser uma pergunta, uma instru√ß√£o ou qualquer sequ√™ncia de texto que forne√ßa contexto para o modelo.

Quando voc√™ faz uma pergunta para o GPT, por exemplo, o texto que voc√™ digita √© chamado de **prompt**. Esse prompt serve como **entrada** para o modelo durante o processo de **infer√™ncia** (gera√ß√£o de texto).

#### Como funciona o prompt no processo de infer√™ncia:

1. **Entrada do Prompt**: O prompt que voc√™ fornece √© a entrada para o modelo, ou seja, a sequ√™ncia de texto que o modelo usa como contexto.

2. **Processamento pelo Decoder**: O modelo ent√£o passa esse prompt atrav√©s do **Decoder**, que, usando os **pesos** que aprendeu durante o treinamento, tenta prever a sequ√™ncia de tokens (palavras, frases) que provavelmente se seguiria ao prompt.

3. **Gera√ß√£o de Resposta**: Com base no prompt e no que foi aprendido durante o treinamento (os pesos e padr√µes de linguagem), o modelo gera uma resposta ou sequ√™ncia de tokens que faz sentido dentro do contexto do prompt fornecido.

Ent√£o, o **prompt** representa o **contexto de entrada** para o modelo, que utiliza o Decoder para gerar uma sa√≠da com base nesse contexto.

### Resumo:

* O GPT-3.5 usa **somente o Decoder** para tanto o pr√©-treinamento quanto a infer√™ncia. Durante o treinamento, o modelo aprende os pesos para prever o pr√≥ximo token com base no contexto dos tokens anteriores.
* O **prompt** √© a **entrada** que voc√™ fornece ao modelo durante a infer√™ncia, e √© utilizado para guiar o modelo a gerar uma resposta apropriada com base no que foi aprendido durante o treinamento.

